{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maekomtrgn/ISDN3000C_Lab07/blob/main/Copy_of_ISDN3000C_Lab07.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSW51Vpuh2VG"
      },
      "source": [
        "# Lab 07: White Box AI\n",
        "\n",
        "You want to make the perfect toast. You have hacked a toaster with sensors and toasted over 1000 toasts. How can we predict the optimal toasting time for a given toast?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONv4ecvRFFsr",
        "outputId": "3fe5a1fa-0caf-44da-df2b-3176d2f2237c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRJxse2XFXtM",
        "outputId": "9cc22815-d5b1-41ff-9c05-b8e99f45a36d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tZ7pkqmpFj2z",
        "outputId": "98b46ea5-11a0-481b-9ac7-bd22a661d0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCOYxA2FKHps"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "Let's first generate and explore the data.\n",
        "\n",
        "We have collected data from 1,000 toasting attempts.\n",
        "\n",
        "Our Data Features:\n",
        "\n",
        "-    ```ToastingTime```: How long the toast was in for (seconds).\n",
        "-    ```BreadThickness```: The thickness of the slice (mm).\n",
        "-    ```IsFrozen```: A button on our toaster (0 for No, 1 for Yes).\n",
        "-    ```AmbientTemp```: The room temperature (Â°C).\n",
        "\n",
        "Our Target Variable:\n",
        "\n",
        "- ```ToastState```: The final result (Under-done, Perfect, or Burnt).\n",
        "\n",
        "\n",
        "Run the code below to generate a pseudo-random dataset. Do not change the seed!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nmrj9XuZBL6i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "def generate_toaster_data_interactive(n_samples=1000):\n",
        "    # Feature Generation\n",
        "    toasting_time = np.random.randint(60, 500, n_samples)\n",
        "    bread_thickness = np.random.uniform(5, 30, n_samples)\n",
        "    is_frozen = np.random.randint(0, 2, n_samples)\n",
        "    ambient_temp = np.random.normal(20, 6, n_samples)\n",
        "\n",
        "    # Non-linear ideal_time\n",
        "    base_ideal_time = (30 + 1.5 * bread_thickness + 0.1 * bread_thickness**2 + 0.005 * bread_thickness**3)\n",
        "    frozen_penalty = is_frozen * (base_ideal_time * (np.exp(0.04 * bread_thickness) - 1))\n",
        "    unadjusted_ideal_time = base_ideal_time + frozen_penalty\n",
        "    temp_efficiency_factor = np.exp(0.025 * (ambient_temp - 20))\n",
        "    ideal_time = unadjusted_ideal_time / temp_efficiency_factor\n",
        "\n",
        "    # Gaussian perfect_score\n",
        "    perfect_window_width = 25 + bread_thickness # Thicker bread is more forgiving\n",
        "    time_diff_sq = (toasting_time - ideal_time)**2\n",
        "    perfect_score = np.exp(-time_diff_sq / (2 * perfect_window_width**2))\n",
        "\n",
        "    # Sigmoid burnt score\n",
        "    burn_onset_offset = 45  # Burning starts to happen ~45s after the ideal time\n",
        "    burn_rate_k = 0.1       # Controls how quickly it goes from not-burnt to burnt\n",
        "    time_past_ideal = toasting_time - ideal_time - burn_onset_offset\n",
        "    burnt_score = 1 / (1 + np.exp(-burn_rate_k * time_past_ideal))\n",
        "\n",
        "    # Add small noise to scores c.\n",
        "    perfect_score += np.random.normal(0, 0.3, n_samples)\n",
        "    burnt_score += np.random.normal(0, 0.3, n_samples)\n",
        "\n",
        "    # Default to 'Under-done'\n",
        "    toast_state = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    # Classify as 'Perfect' if perfect_score is highest AND above a minimum threshold.\n",
        "    is_perfect = (perfect_score > burnt_score) & (perfect_score > 0.6)\n",
        "    toast_state[is_perfect] = 1\n",
        "\n",
        "    # Classify as 'Burnt' if burnt_score is highest AND above a minimum threshold.\n",
        "    is_burnt = (burnt_score > perfect_score) & (burnt_score > 0.5)\n",
        "    toast_state[is_burnt] = 2\n",
        "\n",
        "    # All other data points remain as 'Under-done'.\n",
        "    # This creates non-linear boundaries between all three classes.\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'ToastingTime': toasting_time,\n",
        "        'BreadThickness': bread_thickness,\n",
        "        'IsFrozen': is_frozen,\n",
        "        'AmbientTemp': ambient_temp,\n",
        "        'ToastState': toast_state\n",
        "    })\n",
        "    return df\n",
        "\n",
        "\n",
        "df = generate_toaster_data_interactive()\n",
        "\n",
        "# You can now inspect the new dataframe\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfAC9PjvKvIF"
      },
      "source": [
        "We provide the following utility functions to visualise the distribution of a feature relative to another feature as follows:\n",
        "\n",
        "```displayfeatures(df)``` Displays a matrix of the features plotted against each other\n",
        "\n",
        "1. Each plot where x != y plots the the values of each feature against each other.\n",
        "2. Each plot where x == y represents the distribution of the \"toastiness\" of the bread based on the considered feature.\n",
        "\n",
        "```displayfeatures3D(df)``` Displays the 3 main features agains each other in 3D\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UOA-y8oItGr"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def displayfeatures(df):\n",
        "    features = ['ToastingTime', 'BreadThickness', 'AmbientTemp']\n",
        "    target_col = 'ToastState'\n",
        "    class_labels = {0: 'Under-done', 1: 'Perfect', 2: 'Burnt'}\n",
        "    colors = ['blue', 'green', 'red']\n",
        "\n",
        "    fig, axes = plt.subplots(len(features), len(features), figsize=(12, 12))\n",
        "    fig.suptitle('Visualizing Toaster Data Relationships', fontsize=16)\n",
        "\n",
        "    for i, feature_y in enumerate(features):\n",
        "        for j, feature_x in enumerate(features):\n",
        "            ax = axes[i, j]\n",
        "\n",
        "            # Plot histograms on the diagonal\n",
        "            if i == j:\n",
        "                ax.hist([df[df[target_col]==k][feature_x] for k in class_labels.keys()],\n",
        "                        bins=15, histtype='barstacked', color=colors, label=list(class_labels.values()))\n",
        "            # Plot scatter plots on the off-diagonal\n",
        "            else:\n",
        "                for k in class_labels.keys():\n",
        "                    ax.scatter(df[df[target_col]==k][feature_x],\n",
        "                            df[df[target_col]==k][feature_y],\n",
        "                            s=10,\n",
        "                            alpha=0.5,\n",
        "                            color=colors[k],\n",
        "                            label=class_labels[k])\n",
        "\n",
        "            # Set axis labels\n",
        "            if i==len(features)-1:\n",
        "                ax.set_xlabel(feature_x)\n",
        "            if j==0:\n",
        "                ax.set_ylabel(feature_y)\n",
        "\n",
        "    # Create a single legend for the entire figure\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, loc='upper right')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLlRd1urEiQ_"
      },
      "outputs": [],
      "source": [
        "displayfeatures3D(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciZlV5WQqS6H"
      },
      "source": [
        "## Step 1: Explore the data\n",
        "\n",
        "Check what the data looks like using the two functions defined above, as well as the pandas and matplotlib functions seen during class:\n",
        "\n",
        "- ```df.head()```\n",
        "- ```df.describe()```\n",
        "- boxplots, histograms, scatter plots, etc.\n",
        "\n",
        "The plotting functions ```displayfeatures(df)``` and ```displayfeatures3D(df)``` do not consider whether the bread is frozen or not. You may want to copy these functions to display the data with respect to the bread frozen feature.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EdX8EIpriSs"
      },
      "source": [
        "## Step 2: Making a Train-Test split\n",
        "\n",
        "We need to split the data into two sets:\n",
        "1. a training set that you will use for training the model\n",
        "2. a testing set that will be used ONLY for testing. This testing set will never be integrated in the training, meaning that the model will have never seen that data before testing.\n",
        "\n",
        "The reason for this split is that we want our model to **generalise** to new data rather than memorising the answers. Training with all the data without testing with new data will result in some of the worse **overfitting**.\n",
        "\n",
        "Typically, we tend to use 70% of the data for training and 30% for testing, uniformly distributed across the data. You can extract manually, or use ```sklearn.model_selection.train_test_split```. This function automatically generates four dataframes:\n",
        "- ```X_train``` The *training* dataframe, containing the 'ToastingTime', 'BreadThickness', 'AmbientTemp', 'IsFrozen' features.\n",
        "- ```X_test``` The *testing* dataframe, containing the 'ToastingTime', 'BreadThickness', 'AmbientTemp', 'IsFrozen' features.\n",
        "- ```y_train``` The *training* labels dataframe, containing the 'ToastState' result.\n",
        "- ```y_test``` The *testing* labels dataframe, containing the 'ToastState' result. This is the **ground truth** that you will compare your testing data against."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLlnVgf6uWmd"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[['ToastingTime', 'BreadThickness', 'AmbientTemp', 'IsFrozen']],\n",
        "    df['ToastState'],\n",
        "    test_size=0.3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"-----------------\")\n",
        "print(\"X_train\")\n",
        "print(\"-----------------\")\n",
        "print(X_train.describe())\n",
        "print(\"-----------------\")\n",
        "print(\"X_test\")\n",
        "print(\"-----------------\")\n",
        "print(X_test.describe())\n",
        "print(\"-----------------\")\n",
        "print(\"y_train\")\n",
        "print(\"-----------------\")\n",
        "print(y_train.describe())\n",
        "print(\"-----------------\")\n",
        "print(\"y_test\")\n",
        "print(\"-----------------\")\n",
        "print(y_test.describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OKjkBfOwVRX"
      },
      "source": [
        "## Step 3: Simple Classifiers\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vsACMfLGxTmw"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Let's start with logistic regression. We will use ```sklearn.linear_model.LogisticRegression```. Typical use is as follows:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict values for our testing data\n",
        "predictions = log_reg.predict(X_test)\n",
        "```\n",
        "\n",
        "The ```LogisticRegression``` function can be initialized with some interesting arguments:\n",
        "- ```max_iter``` the maximum number of iterations. Default is 100, but as our data is quite complex, you may have to increase to reach convergence\n",
        "- ```solver``` the solver used to fit the data. Some solvers may be more efficient for some data than others.\n",
        "\n",
        "Once we have trained the Logistic Regression and done predictions, we can check the accuracy as follows:\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate the accuracy between our prediction and the ground truth of our testing data\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "```\n",
        "\n",
        "\n",
        "Documentation is here: https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression and here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\n",
        "#### Tasks\n",
        "\n",
        "1. Train a logistic regressor on your training and testing data.\n",
        "2. Answer the following questions\n",
        "    1. What is the accuracy?\n",
        "    2. Change the max number of iterations. How does the accuracy change?\n",
        "    3. Change the solver. How does the accuracy change? How fast does it converge (use ```log_reg.n_iter_``` to find the number of iterations before convergence)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UH4rwUW8xW7r"
      },
      "outputs": [],
      "source": [
        "# --- TODO: Train a logistic regressor on your training and testing data. --- #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_XAkwdJE1NO"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "Double click the cell and answer the following questions\n",
        "\n",
        "1. What is the accuracy?\n",
        "\n",
        "\n",
        "----\n",
        "2. Change the max number of iterations. How does the accuracy change?\n",
        "\n",
        "\n",
        "----\n",
        "3. Change the solver. How does the accuracy change? How fast does it converge (use ```log_reg.n_iter_``` to find the number of iterations before convergence)?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXOr-6PT6KeU"
      },
      "source": [
        "### Step 4: Decision Tree Classifier\n",
        "\n",
        "Let us move to decision tree models. Typical use of `sklearn.tree.DecisionTreeClassifier` is very similar to other classifiers:\n",
        "\n",
        "```python\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Initialize the Decision Tree model\n",
        "# We can set a random_state for reproducibility of results\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the model\n",
        "dt_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict values\n",
        "predictions = dt_clf.predict(X_test)\n",
        "```\n",
        "\n",
        "The main advantage of a decision tree is its interpretability. However, a major disadvantage is its tendency to **overfit**. A tree that is too deep will learn the noise in the training data and won't generalize well to new data. A key parameter to control this is `max_depth`.\n",
        "\n",
        "Documentation is here: https://scikit-learn.org/stable/modules/tree.html and here: https://scikit-learn.org/stable/api/sklearn.tree.html\n",
        "\n",
        "#### Tasks\n",
        "\n",
        "1.  Train a default `DecisionTreeClassifier` on your training data and evaluate its accuracy on the testing data. How does it compare to the Logistic Regression model?\n",
        "2.  Trees can be visualized! Use the code below to plot the rules your tree has learned.\n",
        "    1.   First, train a new tree but limit its depth by setting `max_depth=3` to make the plot readable.\n",
        "    2.   You can plot the tree using ```plot_tree``` (https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html). Plot the tree.\n",
        "    3. Can you interpret the first couple of decisions the model makes?\n",
        "3.  Investigate overfitting. Train multiple decision trees with `max_depth` varying from 1 to 20. For each depth, calculate and record both the **training accuracy** (predicting on `X_train`) and the **testing accuracy** (predicting on `X_test`). Plot both accuracies on the same graph against `max_depth`.\n",
        "    1.   At what depth does the model start to significantly overfit (i.e., training accuracy keeps increasing while testing accuracy stagnates or drops)?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nKlEpmh7Gvn"
      },
      "outputs": [],
      "source": [
        "# --- TODO: Train a default `DecisionTreeClassifier`, and visualize the tree. --- #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Xy5wYkE1NO"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "Double click the cell and answer the following questions\n",
        "\n",
        "4. write a few words to interpret the first couple of decisions the model makes.\n",
        "\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfAUh-57E1NO"
      },
      "outputs": [],
      "source": [
        "# --- TODO: Investigate overfitting: maybe train multiple decision trees, and explore the overfitting issue. --- #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R--q8b7oE1NO"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "Double click the cell and answer the following questions\n",
        "\n",
        "5. At what depth does the model start to significantly overfit (i.e., training accuracy keeps increasing while testing accuracy stagnates or drops)?\n",
        "\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJbfNqYq9CpU"
      },
      "source": [
        "### Step 5: Support Vector Machines (SVM)\n",
        "\n",
        "Let us use SVM to try to find splits in data that are not necessarily linear. We will use `sklearn.svm.SVC` (Support Vector Classifier). Typical use is as follows:\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Initialize an SVM model with the 'rbf' kernel, and a 'C' hyperparameter at 1\n",
        "svm_clf = SVC(kernel='rbf', C=1.0, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict the values\n",
        "svm_predictions = svm_clf.predict(X_test)\n",
        "\n",
        "# Get the accuracy with respect to the ground truth\n",
        "accuracy = accuracy_score(y_test, svm_predictions)\n",
        "```\n",
        "\n",
        "Documentation is here: https://scikit-learn.org/stable/modules/svm.html and here: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "\n",
        "#### Tasks\n",
        "\n",
        "1.  Train a default `SVC` model and evaluate its accuracy on the test set.\n",
        "2.  The `kernel` is a key parameter. Train and evaluate `SVC` models with the following kernels: `'linear'`, `'poly'`, `'rbf'`, and `'sigmoid'`.\n",
        "    1. Which one performs best for our toast data?\n",
        "    2. Why do you think it is?\n",
        "3.  The `C` parameter is a regularisation parameter. It controls the trade-off between a large margin and a low error rate on the training data. A smaller `C` creates a wider margin but may misclassify more points, while a larger `C` risks creating a more complex, overfitted boundary.\n",
        "    1.   Using the best kernel you found in the previous step, experiment with different values for `C` (e.g., 0.1, 1, 10, 100). How does the accuracy change?\n",
        "4. Another parameter is `gamma`. It dictates the influence of a single training example on the model.  A low gamma value means that each training point affects a wider area, leading to a smoother, more generalized decision boundary. A high gamma value restricts the influence to a smaller, closer range, creating a more complex decision boundary that closely follows the training data, which can lead to overfitting. By default, `gamma` is set to `scale`, which means it scales with the data.\n",
        "    1. Using the best kernel and `C` you found in the previous step, experiment with different values for `gamma` (e.g., 0.1, 1, 10, 100). How does the accuracy change?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZby82L3_P7r"
      },
      "outputs": [],
      "source": [
        "# --- TODO: Train a default SVC and evaluate on test dataset --- #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JeWhWAwE1NP"
      },
      "outputs": [],
      "source": [
        "# --- TODO: Train multiple SVCs with different kernels, and evaluate the results. --- #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMzIzNEnE1NP"
      },
      "outputs": [],
      "source": [
        "# --- TODO: with the best kernel in last step, do more experiments with C parameter --- #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPADqyRiE1NP"
      },
      "outputs": [],
      "source": [
        "# --- TODO: with the best set-up from previous steps, explore parameter gamma. --- #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abpO8kAEMiNC"
      },
      "source": [
        "In practice, we tend to automate the tuning of SVM's hyperparameter. This process involves testing different hyperparameter values to find the combination that minimises loss and maximizes accuracy on a given dataset.\n",
        "\n",
        "Here, we are going to use the GridSearch algorithm to find the best combination of kernel, C, and gamma.\n",
        "\n",
        "Task:\n",
        "1. Run the following script. What is the result? Does it differ from your manual tuning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95N-weMfLeQK"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "\n",
        "# 2. Create a pipeline to scale data and run SVC\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('svm', SVC())\n",
        "])\n",
        "\n",
        "# 3. Define a parameter grid to search\n",
        "# Use a logarithmic scale for C and gamma\n",
        "param_grid = {\n",
        "    'svm__kernel': ['linear','poly','rbf','sigmoid'],\n",
        "    'svm__C': [0.1, 1, 10, 100],\n",
        "    'svm__gamma': [0.001, 0.01, 0.1, 1, 'scale']\n",
        "}\n",
        "\n",
        "# 4. Perform the grid search\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 5. Evaluate the best model\n",
        "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "print(f\"Best cross-validation score: {grid_search.best_score_:.3f}\")\n",
        "print(f\"Test set score with best model: {grid_search.score(X_test, y_test):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5anFJVvE1NP"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "Double click the cell and answer the following questions\n",
        "\n",
        "6. Does it differ from your manual tuning?\n",
        "\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtfBmiHD_tp6"
      },
      "source": [
        "### Step 6: Random Forest\n",
        "\n",
        "Now that we have gone through the single expert models, we can move to ensemble learning with Random Forest.\n",
        "\n",
        "```python\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "rf_predictions = rf_clf.predict(X_test)\n",
        "```\n",
        "\n",
        "Documentation is here: https://scikit-learn.org/stable/modules/ensemble.html#random-forests and here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\n",
        "#### Tasks\n",
        "\n",
        "1.  Train a `RandomForestClassifier` from `sklearn.ensemble` on the data. How does its accuracy compare to the single `DecisionTreeClassifier` and the other models?\n",
        "2.  We can calculate the **feature importance** to show how much each feature contributed to reducing impurity across all the trees.\n",
        "    *   After training your model, access the feature importances using `your_model.feature_importances_`.\n",
        "    *   Plot the importance . Which features are most important for determining the state of the toast? Does this align with your intuition?\n",
        "\n",
        "Hint: to plot the importances, you can use:\n",
        "\n",
        "```python\n",
        "importances = rf_clf.feature_importances_\n",
        "feature_names = X_train.columns\n",
        "forest_importances = pd.Series(importances, index=feature_names)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uCZ_HvbCCv6"
      },
      "outputs": [],
      "source": [
        "# --- TODO: Train a randowm forest classifier  --- #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRcRgT0kE1NP"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "Double click the cell and answer the following questions\n",
        "\n",
        "7. How does its accuracy compare to the single `DecisionTreeClassifier` and the other models?\n",
        "\n",
        "\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBMZ2hLRE1NP"
      },
      "outputs": [],
      "source": [
        "# --- TODO: find and visualize feature_importance of your model  --- #"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQxxGyaXE1NQ"
      },
      "source": [
        "**TODO:**\n",
        "\n",
        "Double click the cell and answer the following questions\n",
        "\n",
        "8. Which features are most important for determining the state of the toast? Does this align with your intuition?\n",
        "\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XM5DKP1tQMSW"
      },
      "source": [
        "### Step 7: XGBoost\n",
        "\n",
        "Finally, let's move to XGBoost. This time, we need to  import it from the xgboost library. Usage is the same as the other libraries.\n",
        "\n",
        "Documentation is here: https://xgboost.readthedocs.io/en/stable/python/python_intro.html\n",
        "\n",
        "#### Task:\n",
        "\n",
        "1.  Train a default `xgboost.XGBClassifier` and evaluate its accuracy. Note that if you get a warning about `use_label_encoder=False`, you can safely ignore it or add the parameter `use_label_encoder=False` to the classifier to suppress it.\n",
        "2.  **Tuning:** XGBoost has many parameters, but two of the most important are `n_estimators` (the number of trees to build) and `learning_rate` (which shrinks the contribution of each tree). A lower learning rate requires more estimators, but can often lead to a more robust model.\n",
        "    1.   Try training a model with more estimators, for example `n_estimators=500`.\n",
        "    2.   Now, try that again but with a lower learning rate, for example `n_estimators=500, learning_rate=0.1`.\n",
        "    3. You can try some other interesting parameters, such as `max_depth` (the max tree depth), or `subsample` (the amount of data passed to a tree).\n",
        "    4.   How do these changes affect accuracy and the risk of overfitting?\n",
        "3.  Like Random Forest, XGBoost can also calculate **feature importance**. This is crucial for understanding what drives the model's predictions.\n",
        "    1.   After training your best XGBoost model, access the feature importances and plot them using the same method as in the Random Forest section.\n",
        "    2.   Compare the feature importances from XGBoost with those from Random Forest. Do the models agree on which features are most important?\n",
        "4. Same as SVM, you can run a GridSearch algorithm to find the optimal parameters. Adapt the script provided in the SVM section to find the optimal learning rate, number of estimators, and any other parameter you can think of.\n",
        "\n",
        "\n",
        "**Create cells of `python` to fill in your code, and of `markdown` to answer the questions.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2Xn-hPhSUe3"
      },
      "source": [
        "### Step 9: Model Comparison and Conclusion\n",
        "\n",
        "You have now trained and evaluated several different classification models, from simple linear models to powerful ensembles. Let's summarize the results to determine which is best for predicting the perfect toast.\n",
        "\n",
        "#### Task\n",
        "\n",
        "1.  Fill in the table below with the best accuracy you achieved for each model type.\n",
        "2.  Answer the following questions:\n",
        "    *   Which model gave the highest accuracy?\n",
        "    *   Which model do you think is the most *interpretable* (easiest to explain to someone)? Why?\n",
        "    *   Considering both performance and interpretability, which model would you choose to deploy in your \"smart toaster\"? Justify your choice.\n",
        "\n",
        "| Model Type | Best Test Accuracy |\n",
        "| :--- | :--- |\n",
        "| Logistic Regression | *your_answer_here* |\n",
        "| Decision Tree | *your_answer_here* |\n",
        "| Support Vector Machine (SVC) | *your_answer_here* |\n",
        "| Random Forest | *your_answer_here* |\n",
        "| XGBoost | *your_answer_here* |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlopJ6XcE1NQ"
      },
      "source": [
        "### Step 10: Fine-tune your best models\n",
        "\n",
        "1.  Please use whatever tools, libraries or methods you know to fine-tune your models.\n",
        "2.  Pick one with the best performance.\n",
        "3.  Write down the trainining and evaluation process about this best model below, with anything that you think is necessary.\n",
        "4.  We will use another blackbox dataset to test your model. The work will be graded based on computing efficiency (runtime) and accuracy."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}